{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Databricks — Alert → Outcome Analysis (Sydney time)\n",
    "\n",
    "Goal: quantify which alerts (by `signal`, `source_tf`) are predictive for forward returns / momentum continuation.\n",
    "\n",
    "This notebook:\n",
    "- maps each alert to the **next OHLC bar** (to avoid lookahead)\n",
    "- computes forward returns at multiple horizons\n",
    "- computes MAE/MFE over a window\n",
    "- computes ATR-normalized outcomes (move in ATRs)\n",
    "- aggregates results by signal, timeframe, and Sydney time buckets (hour/dow)\n",
    "\n",
    "Assumptions:\n",
    "- Tables exist in `workspace.squeeze`: `alerts`, `ohlc`, and `clean_universe`\n",
    "- OHLC timestamps are epoch ms in `open_time`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG `workspace`;\n",
    "USE SCHEMA `squeeze`;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "TZ = 'Australia/Sydney'\n",
    "\n",
    "# Horizons in bars for forward returns (tune per interval)\n",
    "HORIZONS = [1, 4, 12, 24, 48]  # for 1h bars these are 1h,4h,12h,1d,2d\n",
    "MAE_MFE_WINDOW = 48\n",
    "\n",
    "# Alert de-duplication window (ms). Many systems fire repeated alerts close together.\n",
    "DEDUP_WINDOW_MS = 60 * 60 * 1000  # 1 hour\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an alert dataset aligned to OHLC bars\n",
    "We join alerts to OHLC by `(exchange, symbol, interval)` and map each alert to the next bar open_time." 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose interval to analyze first. Start with 1h (recommended)\n",
    "INTERVAL = '1h'\n",
    "\n",
    "univ = (spark.table('clean_universe')\n",
    "  .where(F.col('interval') == INTERVAL)\n",
    "  .select('exchange','symbol','interval')\n",
    ")\n",
    "\n",
    "alerts0 = (spark.table('alerts')\n",
    "  .select('exchange','symbol','signal','source_tf','ts','created_ts')\n",
    "  .withColumn('ts_utc', F.to_timestamp(F.col('ts')/1000))\n",
    "  .withColumn('ts_syd', F.from_utc_timestamp(F.col('ts_utc'), TZ))\n",
    "  .withColumn('hour_syd', F.hour('ts_syd'))\n",
    "  .withColumn('dow_syd', F.date_format('ts_syd', 'E'))\n",
    ")\n",
    "\n",
    "# De-dupe alerts within a time bucket per (exchange,symbol,signal,source_tf)\n",
    "alerts1 = (alerts0\n",
    "  .withColumn('dedup_bucket', (F.col('ts')/F.lit(DEDUP_WINDOW_MS)).cast('bigint'))\n",
    "  .withColumn('rn', F.row_number().over(Window.partitionBy('exchange','symbol','signal','source_tf','dedup_bucket').orderBy('ts')))\n",
    "  .where(F.col('rn') == 1)\n",
    "  .drop('rn')\n",
    ")\n",
    "\n",
    "ohlc0 = (spark.table('ohlc')\n",
    "  .where(F.col('interval') == INTERVAL)\n",
    "  .select('exchange','symbol','interval','open_time','open','high','low','close','volume')\n",
    "  .withColumn('open_dt_utc', F.to_timestamp(F.col('open_time')/1000))\n",
    ")\n",
    "\n",
    "# Keep only clean symbols\n",
    "ohlc1 = ohlc0.join(univ, on=['exchange','symbol','interval'], how='inner')\n",
    "\n",
    "# Map each alert to the next bar open_time (anti-lookahead)\n",
    "# Approach: join on same symbol and select the minimum open_time >= ts\n",
    "# This uses a range join followed by min; for very large data you may want a more optimized approach.\n",
    "alert_to_bar = (alerts1\n",
    "  .join(ohlc1, on=['exchange','symbol'], how='inner')\n",
    "  .where(F.col('open_time') >= F.col('ts'))\n",
    "  .groupBy('exchange','symbol','signal','source_tf','ts','created_ts','ts_utc','ts_syd','hour_syd','dow_syd')\n",
    "  .agg(F.min('open_time').alias('entry_open_time'))\n",
    ")\n",
    "\n",
    "# Join back to OHLC row for entry bar prices\n",
    "ohlc_entry = (ohlc1\n",
    "  .select('exchange','symbol','open_time','open','high','low','close','volume','open_dt_utc')\n",
    "  .withColumnRenamed('open_time','entry_open_time')\n",
    "  .withColumnRenamed('open','entry_open')\n",
    "  .withColumnRenamed('high','entry_high')\n",
    "  .withColumnRenamed('low','entry_low')\n",
    "  .withColumnRenamed('close','entry_close')\n",
    "  .withColumnRenamed('volume','entry_volume')\n",
    "  .withColumnRenamed('open_dt_utc','entry_dt_utc')\n",
    ")\n",
    "\n",
    "entry = (alert_to_bar\n",
    "  .join(ohlc_entry, on=['exchange','symbol','entry_open_time'], how='inner')\n",
    "  .withColumn('entry_dt_syd', F.from_utc_timestamp(F.col('entry_dt_utc'), TZ))\n",
    ")\n",
    "\n",
    "display(entry.limit(20))\n",
    "print('Aligned alerts:', entry.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add forward return horizons + MAE/MFE\n",
    "We’ll compute forward returns relative to `entry_open` at various horizons, plus max favorable/adverse excursion within a window." 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an OHLC table with a bar index per symbol for horizon lookup\n",
    "w = Window.partitionBy('exchange','symbol','interval').orderBy('open_time')\n",
    "ohlc_idx = (ohlc1\n",
    "  .withColumn('bar_idx', F.row_number().over(w) - 1)\n",
    "  .select('exchange','symbol','interval','open_time','bar_idx','open','high','low','close')\n",
    ")\n",
    "\n",
    "entry_idx = (entry\n",
    "  .join(ohlc_idx.select('exchange','symbol','interval','open_time','bar_idx'),\n",
    "        (entry.exchange==ohlc_idx.exchange) & (entry.symbol==ohlc_idx.symbol) & (F.lit(INTERVAL)==ohlc_idx.interval) & (entry.entry_open_time==ohlc_idx.open_time),\n",
    "        how='inner')\n",
    "  .withColumnRenamed('bar_idx', 'entry_bar_idx')\n",
    ")\n",
    "\n",
    "# Forward returns at horizons: join to (entry_bar_idx + h)\n",
    "out = entry_idx\n",
    "for h in HORIZONS:\n",
    "    tgt = (ohlc_idx\n",
    "      .select('exchange','symbol','interval', F.col('bar_idx').alias(f'bar_idx_h{h}'), F.col('close').alias(f'close_h{h}'))\n",
    "    )\n",
    "    out = (out\n",
    "      .join(tgt,\n",
    "            (out.exchange==tgt.exchange) & (out.symbol==tgt.symbol) & (F.lit(INTERVAL)==tgt.interval) & ((out.entry_bar_idx + F.lit(h))==F.col(f'bar_idx_h{h}')),\n",
    "            how='left')\n",
    "      .withColumn(f'ret_h{h}', (F.col(f'close_h{h}')/F.col('entry_open')) - F.lit(1.0))\n",
    "    )\n",
    "\n",
    "# MAE/MFE over a window of bars after entry\n",
    "# Join bars in [entry_idx, entry_idx+W] and aggregate max high / min low\n",
    "bars = (out\n",
    "  .select('exchange','symbol','interval','signal','source_tf','ts','entry_bar_idx','entry_open','hour_syd','dow_syd')\n",
    "  .join(ohlc_idx, on=['exchange','symbol','interval'], how='inner')\n",
    "  .where((F.col('bar_idx') >= F.col('entry_bar_idx')) & (F.col('bar_idx') <= (F.col('entry_bar_idx') + F.lit(MAE_MFE_WINDOW))))\n",
    ")\n",
    "\n",
    "mae_mfe = (bars\n",
    "  .groupBy('exchange','symbol','interval','signal','source_tf','ts','entry_bar_idx','entry_open','hour_syd','dow_syd')\n",
    "  .agg(\n",
    "    F.max('high').alias('max_high_w'),\n",
    "    F.min('low').alias('min_low_w')\n",
    "  )\n",
    "  .withColumn('mfe', (F.col('max_high_w')/F.col('entry_open')) - 1.0)\n",
    "  .withColumn('mae', (F.col('min_low_w')/F.col('entry_open')) - 1.0)\n",
    ")\n",
    "\n",
    "out2 = (out\n",
    "  .join(mae_mfe, on=['exchange','symbol','interval','signal','source_tf','ts','entry_bar_idx','entry_open','hour_syd','dow_syd'], how='left')\n",
    ")\n",
    "\n",
    "display(out2.select('exchange','symbol','signal','source_tf','entry_open','ret_h1','ret_h4','ret_h24','mfe','mae','hour_syd','dow_syd').limit(50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate: win rates and distribution summaries\n",
    "We’ll compute hit-rate for positive returns and quantiles by signal/timeframe and Sydney hour/dow." 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_for_h(h: int):\n",
    "    return [\n",
    "        F.count('*').alias('n'),\n",
    "        F.avg(F.when(F.col(f'ret_h{h}') > 0, 1.0).otherwise(0.0)).alias('hit_rate_pos'),\n",
    "        F.avg(F.col(f'ret_h{h}')).alias('avg_ret'),\n",
    "        F.expr(f'percentile_approx(ret_h{h}, 0.5)').alias('median_ret'),\n",
    "        F.expr(f'percentile_approx(ret_h{h}, 0.1)').alias('p10_ret'),\n",
    "        F.expr(f'percentile_approx(ret_h{h}, 0.9)').alias('p90_ret'),\n",
    "    ]\n",
    "\n",
    "# Example: horizon 24 bars\n",
    "H = 24\n",
    "agg = (out2\n",
    "  .groupBy('signal','source_tf')\n",
    "  .agg(*agg_for_h(H),\n",
    "       F.avg('mfe').alias('avg_mfe'),\n",
    "       F.avg('mae').alias('avg_mae'))\n",
    "  .orderBy(F.col('avg_ret').desc())\n",
    ")\n",
    "display(agg.limit(200))\n",
    "\n",
    "# Session effects (Sydney)\n",
    "agg_sess = (out2\n",
    "  .groupBy('signal','source_tf','dow_syd','hour_syd')\n",
    "  .agg(*agg_for_h(H))\n",
    "  .where(F.col('n') >= 50)\n",
    "  .orderBy(F.col('avg_ret').desc())\n",
    ")\n",
    "display(agg_sess.limit(200))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
