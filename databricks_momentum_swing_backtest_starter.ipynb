{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Databricks â€” Momentum Swing Backtest Starter (Sydney reporting)\n",
    "\n",
    "This notebook supports two workflows:\n",
    "1) **Single-symbol iteration** (fast signal development)\n",
    "2) **Multi-symbol backtest** at scale using Spark `groupBy(...).applyInPandas(...)`\n",
    "\n",
    "Timezone:\n",
    "- Data is stored as epoch ms. We convert to UTC timestamps, and add a Sydney column for reporting (AEST/AEDT DST-aware).\n",
    "- Strategy logic should generally use UTC ordering to avoid DST edge cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG `workspace`;\n",
    "USE SCHEMA `squeeze`;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "TZ = 'Australia/Sydney'\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.width', 140)\n",
    "\n",
    "RUN_ID = uuid.uuid4().hex\n",
    "print('RUN_ID =', RUN_ID)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy parameters" 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BacktestParams:\n",
    "    atr_mult_stop: float = 2.0\n",
    "    atr_mult_tp: float = 3.0\n",
    "    max_hold_bars: int = 48\n",
    "\n",
    "PARAMS = BacktestParams(atr_mult_stop=2.0, atr_mult_tp=3.0, max_hold_bars=48)\n",
    "SIGNALS = ['sig_breakout_long']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-symbol iteration (optional)\n",
    "Use this section to iterate quickly on features/signals for one symbol, then run the multi-symbol section." 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCHANGE = 'binance'\n",
    "SYMBOL = 'BTCUSDT'\n",
    "INTERVAL = '1h'\n",
    "LIMIT_ROWS = 20000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohlc_s = (spark.table('ohlc')\n",
    "  .where((F.col('exchange')==EXCHANGE) & (F.col('symbol')==SYMBOL) & (F.col('interval')==INTERVAL))\n",
    "  .select('exchange','symbol','interval','open_time','close_time','open','high','low','close','volume')\n",
    "  .withColumn('open_dt_utc', F.to_timestamp(F.col('open_time')/1000))\n",
    "  .withColumn('open_dt_syd', F.from_utc_timestamp(F.col('open_dt_utc'), TZ))\n",
    "  .orderBy('open_time')\n",
    "  .limit(LIMIT_ROWS)\n",
    ")\n",
    "display(ohlc_s.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ohlc_s.toPandas()\n",
    "for c in ['open','high','low','close','volume']:\n",
    "    df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "df = df.sort_values('open_time').reset_index(drop=True)\n",
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ema(s: pd.Series, span: int) -> pd.Series:\n",
    "    return s.ewm(span=span, adjust=False).mean()\n",
    "\n",
    "def atr(high: pd.Series, low: pd.Series, close: pd.Series, n: int = 14) -> pd.Series:\n",
    "    prev_close = close.shift(1)\n",
    "    tr = pd.concat([(high - low).abs(), (high - prev_close).abs(), (low - prev_close).abs()], axis=1).max(axis=1)\n",
    "    return tr.rolling(n, min_periods=n).mean()\n",
    "\n",
    "def add_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out['ema_20'] = ema(out['close'], 20)\n",
    "    out['ema_50'] = ema(out['close'], 50)\n",
    "    out['trend_up'] = out['ema_20'] > out['ema_50']\n",
    "    out['atr_14'] = atr(out['high'], out['low'], out['close'], 14)\n",
    "    out['atrp_14'] = out['atr_14'] / out['close']\n",
    "    L = 20\n",
    "    out['hh_20'] = out['high'].rolling(L, min_periods=L).max()\n",
    "    out['ll_20'] = out['low'].rolling(L, min_periods=L).min()\n",
    "    return out\n",
    "\n",
    "def add_signals(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out['sig_breakout_long'] = out['trend_up'] & (out['close'] > out['hh_20'].shift(1))\n",
    "    return out\n",
    "\n",
    "df = add_signals(add_features(df))\n",
    "df[['open_dt_syd','close','ema_20','ema_50','atr_14','hh_20','sig_breakout_long']].tail(50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtest core (pandas)\n",
    "Used both for single-symbol iteration and in Spark `applyInPandas`." 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_long(df: pd.DataFrame, signal_col: str, p: BacktestParams) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    n = len(df)\n",
    "    for i in range(n - 2):\n",
    "        if not bool(df[signal_col].iloc[i]):\n",
    "            continue\n",
    "        entry_i = i + 1  # enter next bar open (avoid lookahead)\n",
    "        entry = float(df['open'].iloc[entry_i])\n",
    "        atrv = float(df['atr_14'].iloc[entry_i])\n",
    "        if not np.isfinite(entry) or not np.isfinite(atrv) or atrv <= 0:\n",
    "            continue\n",
    "        stop = entry - p.atr_mult_stop * atrv\n",
    "        tp = entry + p.atr_mult_tp * atrv\n",
    "        last_i = min(n - 1, entry_i + p.max_hold_bars)\n",
    "\n",
    "        exit_i = None\n",
    "        exit_px = None\n",
    "        outcome = None\n",
    "\n",
    "        # Conservative intrabar ordering for long: stop can trigger before TP within bar\n",
    "        for j in range(entry_i, last_i + 1):\n",
    "            lo = float(df['low'].iloc[j])\n",
    "            hi = float(df['high'].iloc[j])\n",
    "            if lo <= stop:\n",
    "                exit_i, exit_px, outcome = j, stop, 'stop'\n",
    "                break\n",
    "            if hi >= tp:\n",
    "                exit_i, exit_px, outcome = j, tp, 'tp'\n",
    "                break\n",
    "\n",
    "        if exit_i is None:\n",
    "            exit_i, exit_px, outcome = last_i, float(df['close'].iloc[last_i]), 'time'\n",
    "\n",
    "        r = (exit_px - entry) / (entry - stop) if (entry - stop) != 0 else np.nan\n",
    "\n",
    "        rows.append({\n",
    "            'entry_i': entry_i,\n",
    "            'exit_i': exit_i,\n",
    "            'entry_time_utc': df['open_dt_utc'].iloc[entry_i],\n",
    "            'entry_time_syd': df['open_dt_syd'].iloc[entry_i],\n",
    "            'exit_time_utc': df['open_dt_utc'].iloc[exit_i],\n",
    "            'entry': entry,\n",
    "            'stop': stop,\n",
    "            'tp': tp,\n",
    "            'exit': exit_px,\n",
    "            'bars_held': int(exit_i - entry_i),\n",
    "            'outcome': outcome,\n",
    "            'r_multiple': float(r),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def summarize(trades: pd.DataFrame) -> pd.DataFrame:\n",
    "    if trades.empty:\n",
    "        return pd.DataFrame([{'n_trades': 0}])\n",
    "    wins = trades['r_multiple'] > 0\n",
    "    return pd.DataFrame([{\n",
    "        'n_trades': int(len(trades)),\n",
    "        'win_rate': float(wins.mean()),\n",
    "        'avg_r': float(trades['r_multiple'].mean()),\n",
    "        'median_r': float(trades['r_multiple'].median()),\n",
    "        'p10_r': float(trades['r_multiple'].quantile(0.10)),\n",
    "        'p90_r': float(trades['r_multiple'].quantile(0.90)),\n",
    "        'avg_bars_held': float(trades['bars_held'].mean()),\n",
    "    }])\n",
    "\n",
    "# Single-symbol quick check\n",
    "trades = backtest_long(df, 'sig_breakout_long', PARAMS)\n",
    "display(trades.head(20))\n",
    "display(summarize(trades))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-symbol backtest (Spark)\n",
    "This section backtests a whole universe by grouping OHLC per `(exchange, symbol, interval)` and running your pandas logic with `applyInPandas`.\n",
    "\n",
    "Tip: Start with a small universe (e.g., top 20 by recent volume) until the strategy logic is stable." 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIVERSE_TOP_N = 30\n",
    "UNIVERSE_INTERVAL = '1h'\n",
    "MIN_BARS = 500\n",
    "\n",
    "# Optional: limit to recent history for volume ranking (tune)\n",
    "# Example: last ~180 days in ms\n",
    "DAYS_FOR_RANK = 180\n",
    "MS_PER_DAY = 86400000\n",
    "now_ms = int(spark.sql('SELECT CAST(unix_millis(current_timestamp()) AS BIGINT) AS now_ms').collect()[0]['now_ms'])\n",
    "min_open_time = now_ms - DAYS_FOR_RANK * MS_PER_DAY\n",
    "\n",
    "base = (spark.table('ohlc')\n",
    "  .where(F.col('interval') == UNIVERSE_INTERVAL)\n",
    "  .where(F.col('open_time') >= F.lit(min_open_time))\n",
    "  .select('exchange','symbol','interval','open_time','close_time','open','high','low','close','volume')\n",
    ")\n",
    "\n",
    "universe = (base\n",
    "  .groupBy('exchange','symbol','interval')\n",
    "  .agg(\n",
    "    F.sum(F.col('volume').cast('double')).alias('vol_sum'),\n",
    "    F.count('*').alias('bars')\n",
    "  )\n",
    "  .where(F.col('bars') >= MIN_BARS)\n",
    "  .orderBy(F.col('vol_sum').desc())\n",
    "  .limit(UNIVERSE_TOP_N)\n",
    ")\n",
    "\n",
    "display(universe)\n",
    "print('Universe size:', universe.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run grouped backtest via applyInPandas" 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema for trade-level output written to Delta\n",
    "trade_schema = T.StructType([\n",
    "    T.StructField('run_id', T.StringType(), False),\n",
    "    T.StructField('exchange', T.StringType(), True),\n",
    "    T.StructField('symbol', T.StringType(), True),\n",
    "    T.StructField('interval', T.StringType(), True),\n",
    "    T.StructField('signal', T.StringType(), True),\n",
    "    T.StructField('entry_time_utc', T.TimestampType(), True),\n",
    "    T.StructField('entry_time_syd', T.TimestampType(), True),\n",
    "    T.StructField('exit_time_utc', T.TimestampType(), True),\n",
    "    T.StructField('entry', T.DoubleType(), True),\n",
    "    T.StructField('stop', T.DoubleType(), True),\n",
    "    T.StructField('tp', T.DoubleType(), True),\n",
    "    T.StructField('exit', T.DoubleType(), True),\n",
    "    T.StructField('bars_held', T.IntegerType(), True),\n",
    "    T.StructField('outcome', T.StringType(), True),\n",
    "    T.StructField('r_multiple', T.DoubleType(), True),\n",
    "])\n",
    "\n",
    "def backtest_group(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    # pdf contains one (exchange,symbol,interval) group\n",
    "    if pdf.empty:\n",
    "        return pd.DataFrame(columns=[f.name for f in trade_schema.fields])\n",
    "\n",
    "    # ensure ordering and dtypes\n",
    "    pdf = pdf.sort_values('open_time').reset_index(drop=True)\n",
    "    for c in ['open','high','low','close','volume']:\n",
    "        pdf[c] = pd.to_numeric(pdf[c], errors='coerce')\n",
    "\n",
    "    # time columns\n",
    "    pdf['open_dt_utc'] = pd.to_datetime(pd.to_numeric(pdf['open_time'], errors='coerce'), unit='ms', utc=True)\n",
    "    pdf['open_dt_syd'] = pdf['open_dt_utc'].dt.tz_convert(TZ)\n",
    "\n",
    "    pdf = add_signals(add_features(pdf))\n",
    "\n",
    "    out_rows = []\n",
    "    for sig in SIGNALS:\n",
    "        tr = backtest_long(pdf, sig, PARAMS)\n",
    "        if tr.empty:\n",
    "            continue\n",
    "        # enrich\n",
    "        ex = pdf['exchange'].iloc[0] if 'exchange' in pdf.columns else None\n",
    "        sym = pdf['symbol'].iloc[0] if 'symbol' in pdf.columns else None\n",
    "        itv = pdf['interval'].iloc[0] if 'interval' in pdf.columns else None\n",
    "        tr.insert(0, 'signal', sig)\n",
    "        tr.insert(0, 'interval', itv)\n",
    "        tr.insert(0, 'symbol', sym)\n",
    "        tr.insert(0, 'exchange', ex)\n",
    "        tr.insert(0, 'run_id', RUN_ID)\n",
    "        out_rows.append(tr[['run_id','exchange','symbol','interval','signal',\n",
    "                            'entry_time_utc','entry_time_syd','exit_time_utc',\n",
    "                            'entry','stop','tp','exit','bars_held','outcome','r_multiple']])\n",
    "\n",
    "    if not out_rows:\n",
    "        return pd.DataFrame(columns=[f.name for f in trade_schema.fields])\n",
    "    return pd.concat(out_rows, ignore_index=True)\n",
    "\n",
    "ohlc_universe = (spark.table('ohlc')\n",
    "  .join(universe.select('exchange','symbol','interval'), on=['exchange','symbol','interval'], how='inner')\n",
    "  .select('exchange','symbol','interval','open_time','close_time','open','high','low','close','volume')\n",
    ")\n",
    "\n",
    "trades_s = (ohlc_universe\n",
    "  .groupBy('exchange','symbol','interval')\n",
    "  .applyInPandas(backtest_group, schema=trade_schema)\n",
    ")\n",
    "\n",
    "display(trades_s.limit(50))\n",
    "print('Trade rows:', trades_s.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write outputs to Delta + compute summary stats" 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure tables exist (Delta). You can change names if you prefer.\n",
    "spark.sql('''\n",
    "CREATE TABLE IF NOT EXISTS backtest_trades (\n",
    "  run_id STRING,\n",
    "  exchange STRING,\n",
    "  symbol STRING,\n",
    "  interval STRING,\n",
    "  signal STRING,\n",
    "  entry_time_utc TIMESTAMP,\n",
    "  entry_time_syd TIMESTAMP,\n",
    "  exit_time_utc TIMESTAMP,\n",
    "  entry DOUBLE,\n",
    "  stop DOUBLE,\n",
    "  tp DOUBLE,\n",
    "  exit DOUBLE,\n",
    "  bars_held INT,\n",
    "  outcome STRING,\n",
    "  r_multiple DOUBLE\n",
    ") USING DELTA\n",
    "''')\n",
    "\n",
    "spark.sql('''\n",
    "CREATE TABLE IF NOT EXISTS backtest_results (\n",
    "  run_id STRING,\n",
    "  exchange STRING,\n",
    "  symbol STRING,\n",
    "  interval STRING,\n",
    "  signal STRING,\n",
    "  n_trades BIGINT,\n",
    "  win_rate DOUBLE,\n",
    "  avg_r DOUBLE,\n",
    "  median_r DOUBLE,\n",
    "  p10_r DOUBLE,\n",
    "  p90_r DOUBLE,\n",
    "  avg_bars_held DOUBLE\n",
    ") USING DELTA\n",
    "''')\n",
    "\n",
    "# Append trades\n",
    "(trades_s\n",
    "  .write\n",
    "  .mode('append')\n",
    "  .saveAsTable('backtest_trades')\n",
    ")\n",
    "\n",
    "# Build summary per symbol+signal\n",
    "results_s = (trades_s\n",
    "  .groupBy('run_id','exchange','symbol','interval','signal')\n",
    "  .agg(\n",
    "    F.count('*').alias('n_trades'),\n",
    "    F.avg(F.when(F.col('r_multiple') > 0, 1.0).otherwise(0.0)).alias('win_rate'),\n",
    "    F.avg('r_multiple').alias('avg_r'),\n",
    "    F.expr('percentile_approx(r_multiple, 0.5)').alias('median_r'),\n",
    "    F.expr('percentile_approx(r_multiple, 0.1)').alias('p10_r'),\n",
    "    F.expr('percentile_approx(r_multiple, 0.9)').alias('p90_r'),\n",
    "    F.avg('bars_held').alias('avg_bars_held'),\n",
    "  )\n",
    ")\n",
    "\n",
    "(results_s\n",
    "  .write\n",
    "  .mode('append')\n",
    "  .saveAsTable('backtest_results')\n",
    ")\n",
    "\n",
    "display(results_s.orderBy(F.col('avg_r').desc()).limit(100))\n",
    "print('Wrote run_id', RUN_ID)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
