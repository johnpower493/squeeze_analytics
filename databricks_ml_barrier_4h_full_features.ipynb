{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Databricks ML — 4h Per-Bar Barrier Classifier (Full Feature Engineering)\n",
    "\n",
    "- Grain: **every 4h bar** (per-bar dataset)\n",
    "- Label: **barrier outcome** (TP hit before SL within W bars)\n",
    "\n",
    "This notebook builds a supervised dataset from `ohlc` (filtered by `clean_universe`) and trains a classifier in Spark ML.\n",
    "\n",
    "Tables assumed (Unity Catalog):\n",
    "- `workspace.squeeze.ohlc`\n",
    "- `workspace.squeeze.clean_universe`\n",
    "\n",
    "Outputs:\n",
    "- `ml_barrier_dataset_4h` (features + label)\n",
    "- `ml_barrier_predictions_4h` (scored rows with probability)\n",
    "\n",
    "Notes:\n",
    "- **No lookahead**: all features are lagged/rolling with correct window framing.\n",
    "- **Sydney time** is included for time-of-day features, but ordering uses UTC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG `workspace`;\n",
    "USE SCHEMA `squeeze`;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "TZ = 'Australia/Sydney'\n",
    "RUN_ID = uuid.uuid4().hex\n",
    "print('RUN_ID =', RUN_ID)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERVAL = '4h'\n",
    "UNIVERSE_LIMIT = 300  # set None for full clean_universe\n",
    "\n",
    "# Barrier labeling parameters\n",
    "ATR_N = 14\n",
    "TP_ATR = 3.0\n",
    "SL_ATR = 2.0\n",
    "W_BARS = 24  # lookahead window in bars (24*4h = 4 days)\n",
    "\n",
    "# OHLC ambiguity policy when both TP and SL are hit in the *same bar*.\n",
    "# Options:\n",
    "# - 'tp_first'     : count as win\n",
    "# - 'sl_first'     : count as loss\n",
    "# - 'discard_both' : set label NULL (remove ambiguous samples)\n",
    "AMBIGUITY_POLICY = 'discard_both'\n",
    "\n",
    "\n",
    "# Feature windows\n",
    "EMA_FAST = 20\n",
    "EMA_SLOW = 50\n",
    "ROLL_HH_N = 20\n",
    "ROLL_LL_N = 20\n",
    "RET_VOL_N = 20\n",
    "\n",
    "# Split boundaries (UTC timestamps). Tune as you like.\n",
    "TRAIN_END = '2024-01-01'\n",
    "VALID_END = '2025-01-01'\n",
    "\n",
    "# Sampling / class balance\n",
    "MAX_ROWS_PER_SYMBOL = None  # optionally cap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load OHLC for clean universe (4h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe = (spark.table('clean_universe')\n",
    "  .where(F.col('interval') == INTERVAL)\n",
    "  .select('exchange','symbol','interval')\n",
    ")\n",
    "if UNIVERSE_LIMIT is not None:\n",
    "  universe = universe.limit(int(UNIVERSE_LIMIT))\n",
    "\n",
    "ohlc = (spark.table('ohlc')\n",
    "  .join(universe, on=['exchange','symbol','interval'], how='inner')\n",
    "  .select('exchange','symbol','interval','open_time','open','high','low','close','volume')\n",
    "  .withColumn('open_dt_utc', F.to_timestamp(F.col('open_time')/1000))\n",
    "  .withColumn('open_dt_syd', F.from_utc_timestamp(F.col('open_dt_utc'), TZ))\n",
    "  .withColumn('hour_syd', F.hour('open_dt_syd'))\n",
    "  .withColumn('dow_syd', F.date_format('open_dt_syd', 'E'))\n",
    ")\n",
    "\n",
    "display(ohlc.limit(5))\n",
    "print('rows:', ohlc.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering in Spark (no leakage)\n",
    "We compute lagged/rolling features using window frames that end at the current row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy('exchange','symbol','interval').orderBy('open_time')\n",
    "w_prev = w.rowsBetween(-1000000, -1)  # 'all history up to prev row' for some ops\n",
    "\n",
    "# Basic returns\n",
    "feat = (ohlc\n",
    "  .withColumn('close_d', F.col('close').cast('double'))\n",
    "  .withColumn('open_d', F.col('open').cast('double'))\n",
    "  .withColumn('high_d', F.col('high').cast('double'))\n",
    "  .withColumn('low_d', F.col('low').cast('double'))\n",
    "  .withColumn('vol_d', F.col('volume').cast('double'))\n",
    "  .withColumn('prev_close', F.lag('close_d', 1).over(w))\n",
    "  .withColumn('ret_1', F.log(F.col('close_d') / F.col('prev_close')))\n",
    "  .withColumn('range', F.col('high_d') - F.col('low_d'))\n",
    "  .withColumn('body', F.abs(F.col('close_d') - F.col('open_d')))\n",
    "  .withColumn('upper_wick', F.col('high_d') - F.greatest(F.col('open_d'), F.col('close_d')))\n",
    "  .withColumn('lower_wick', F.least(F.col('open_d'), F.col('close_d')) - F.col('low_d'))\n",
    "  .withColumn('body_to_range', F.when(F.col('range') > 0, F.col('body')/F.col('range')).otherwise(F.lit(None)))\n",
    "  .withColumn('gap', F.col('open_d') - F.col('prev_close'))\n",
    ")\n",
    "\n",
    "# ATR (simple moving average of true range)\n",
    "tr = F.greatest(\n",
    "  (F.col('high_d') - F.col('low_d')).cast('double'),\n",
    "  F.abs(F.col('high_d') - F.col('prev_close')),\n",
    "  F.abs(F.col('low_d') - F.col('prev_close'))\n",
    ")\n",
    "feat = (feat\n",
    "  .withColumn('tr', tr)\n",
    "  .withColumn('atr', F.avg('tr').over(w.rowsBetween(-(ATR_N-1), 0)))\n",
    "  .withColumn('atrp', F.col('atr') / F.col('close_d'))\n",
    ")\n",
    "\n",
    "# EMAs via exponential smoothing implemented with Spark SQL function `ewm` is not available;\n",
    "# Instead we use moving averages as a strong baseline. You can replace with pandas UDF EMAs later.\n",
    "feat = (feat\n",
    "  .withColumn('sma_fast', F.avg('close_d').over(w.rowsBetween(-(EMA_FAST-1), 0)))\n",
    "  .withColumn('sma_slow', F.avg('close_d').over(w.rowsBetween(-(EMA_SLOW-1), 0)))\n",
    "  .withColumn('trend_up', (F.col('sma_fast') > F.col('sma_slow')).cast('int'))\n",
    "  .withColumn('trend_strength', (F.col('sma_fast') - F.col('sma_slow')) / F.col('atr'))\n",
    ")\n",
    "\n",
    "# Rolling highs/lows (shifted by 1 to avoid lookahead for breakout context)\n",
    "feat = (feat\n",
    "  .withColumn('hh_n', F.max('high_d').over(w.rowsBetween(-(ROLL_HH_N), -1)))\n",
    "  .withColumn('ll_n', F.min('low_d').over(w.rowsBetween(-(ROLL_LL_N), -1)))\n",
    "  .withColumn('dist_to_hh_atr', (F.col('close_d') - F.col('hh_n')) / F.col('atr'))\n",
    "  .withColumn('dist_to_ll_atr', (F.col('close_d') - F.col('ll_n')) / F.col('atr'))\n",
    ")\n",
    "\n",
    "# Volatility and volume regime\n",
    "feat = (feat\n",
    "  .withColumn('ret_vol', F.stddev('ret_1').over(w.rowsBetween(-(RET_VOL_N-1), 0)))\n",
    "  .withColumn('range_sma', F.avg('range').over(w.rowsBetween(-(RET_VOL_N-1), 0)))\n",
    "  .withColumn('vol_sma', F.avg('vol_d').over(w.rowsBetween(-(RET_VOL_N-1), 0)))\n",
    "  .withColumn('vol_std', F.stddev('vol_d').over(w.rowsBetween(-(RET_VOL_N-1), 0)))\n",
    "  .withColumn('vol_z', F.when(F.col('vol_std') > 0, (F.col('vol_d')-F.col('vol_sma'))/F.col('vol_std')).otherwise(F.lit(None)))\n",
    ")\n",
    "\n",
    "# Add a bar index for forward barrier label joins\n",
    "feat = feat.withColumn('bar_idx', (F.row_number().over(w) - 1).cast('long'))\n",
    "\n",
    "display(feat.select('exchange','symbol','open_dt_syd','close','atr','atrp','trend_up','dist_to_hh_atr','vol_z').limit(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced technical features (EMA/RSI/ADX) via applyInPandas\n",
    "Spark doesn’t provide native EMA/RSI/ADX operators. For stronger features we compute them per symbol using `groupBy(...).applyInPandas(...)`.\n",
    "\n",
    "This keeps the pipeline scalable while preserving true indicator definitions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "def _ema(s: pd.Series, span: int) -> pd.Series:\n",
    "    return s.ewm(span=span, adjust=False).mean()\n",
    "\n",
    "def _rsi(close: pd.Series, n: int = 14) -> pd.Series:\n",
    "    delta = close.diff()\n",
    "    up = delta.clip(lower=0)\n",
    "    down = (-delta).clip(lower=0)\n",
    "    roll_up = up.ewm(alpha=1/n, adjust=False).mean()\n",
    "    roll_down = down.ewm(alpha=1/n, adjust=False).mean()\n",
    "    rs = roll_up / roll_down\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def _adx(high: pd.Series, low: pd.Series, close: pd.Series, n: int = 14) -> pd.Series:\n",
    "    high_shift = high.shift(1)\n",
    "    low_shift = low.shift(1)\n",
    "    close_shift = close.shift(1)\n",
    "    up_move = high - high_shift\n",
    "    down_move = low_shift - low\n",
    "    plus_dm = np.where((up_move > down_move) & (up_move > 0), up_move, 0.0)\n",
    "    minus_dm = np.where((down_move > up_move) & (down_move > 0), down_move, 0.0)\n",
    "    tr = pd.concat([(high - low).abs(), (high - close_shift).abs(), (low - close_shift).abs()], axis=1).max(axis=1)\n",
    "    atr = tr.ewm(alpha=1/n, adjust=False).mean()\n",
    "    plus_di = 100 * (pd.Series(plus_dm, index=high.index).ewm(alpha=1/n, adjust=False).mean() / atr)\n",
    "    minus_di = 100 * (pd.Series(minus_dm, index=high.index).ewm(alpha=1/n, adjust=False).mean() / atr)\n",
    "    dx = (100 * (plus_di - minus_di).abs() / (plus_di + minus_di)).replace([np.inf, -np.inf], np.nan)\n",
    "    adx = dx.ewm(alpha=1/n, adjust=False).mean()\n",
    "    return adx\n",
    "\n",
    "def _bb(close: pd.Series, n: int = 20, k: float = 2.0):\n",
    "    mid = close.rolling(n, min_periods=n).mean()\n",
    "    sd = close.rolling(n, min_periods=n).std()\n",
    "    upper = mid + k*sd\n",
    "    lower = mid - k*sd\n",
    "    bw = (upper - lower) / mid\n",
    "    return bw\n",
    "\n",
    "def _macd(close: pd.Series, fast: int = 12, slow: int = 26, sig: int = 9):\n",
    "    macd_line = _ema(close, fast) - _ema(close, slow)\n",
    "    signal = _ema(macd_line, sig)\n",
    "    hist = macd_line - signal\n",
    "    return macd_line, signal, hist\n",
    "\n",
    "tech_schema = T.StructType([\n",
    "  T.StructField('exchange', T.StringType()),\n",
    "  T.StructField('symbol', T.StringType()),\n",
    "  T.StructField('interval', T.StringType()),\n",
    "  T.StructField('open_time', T.LongType()),\n",
    "  T.StructField('ema_20', T.DoubleType()),\n",
    "  T.StructField('ema_50', T.DoubleType()),\n",
    "  T.StructField('ema_ratio', T.DoubleType()),\n",
    "  T.StructField('rsi_14', T.DoubleType()),\n",
    "  T.StructField('adx_14', T.DoubleType()),\n",
    "  T.StructField('macd_line', T.DoubleType()),\n",
    "  T.StructField('macd_signal', T.DoubleType()),\n",
    "  T.StructField('macd_hist', T.DoubleType()),\n",
    "  T.StructField('bb_bw_20', T.DoubleType()),\n",
    "  T.StructField('donch_pos_20', T.DoubleType()),\n",
    "  T.StructField('mom_logret_3', T.DoubleType()),\n",
    "  T.StructField('mom_logret_6', T.DoubleType()),\n",
    "  T.StructField('mom_logret_12', T.DoubleType()),\n",
    "])\n",
    "\n",
    "def compute_tech(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "  pdf = pdf.sort_values('open_time').reset_index(drop=True)\n",
    "  for c in ['close_d','high_d','low_d']:\n",
    "    pdf[c] = pd.to_numeric(pdf[c], errors='coerce')\n",
    "  close = pdf['close_d']\n",
    "  ema20 = _ema(close, 20)\n",
    "  ema50 = _ema(close, 50)\n",
    "  rsi14 = _rsi(close, 14)\n",
    "  adx14 = _adx(pdf['high_d'], pdf['low_d'], close, 14)\n",
    "  macd_line, macd_sig, macd_hist = _macd(close)\n",
    "  bb_bw = _bb(close, 20, 2.0)\n",
    "  hh = pdf['high_d'].rolling(20, min_periods=20).max()\n",
    "  ll = pdf['low_d'].rolling(20, min_periods=20).min()\n",
    "  donch_pos = (close - ll) / (hh - ll)\n",
    "  mom3 = np.log(close / close.shift(3))\n",
    "  mom6 = np.log(close / close.shift(6))\n",
    "  mom12 = np.log(close / close.shift(12))\n",
    "  out = pdf[['exchange','symbol','interval','open_time']].copy()\n",
    "  out['ema_20'] = ema20.astype('float64')\n",
    "  out['ema_50'] = ema50.astype('float64')\n",
    "  out['ema_ratio'] = (ema20 / ema50).astype('float64')\n",
    "  out['rsi_14'] = rsi14.astype('float64')\n",
    "  out['adx_14'] = adx14.astype('float64')\n",
    "  out['macd_line'] = macd_line.astype('float64')\n",
    "  out['macd_signal'] = macd_sig.astype('float64')\n",
    "  out['macd_hist'] = macd_hist.astype('float64')\n",
    "  out['bb_bw_20'] = bb_bw.astype('float64')\n",
    "  out['donch_pos_20'] = donch_pos.astype('float64')\n",
    "  out['mom_logret_3'] = mom3.astype('float64')\n",
    "  out['mom_logret_6'] = mom6.astype('float64')\n",
    "  out['mom_logret_12'] = mom12.astype('float64')\n",
    "  return out\n",
    "\n",
    "tech = (feat\n",
    "  .select('exchange','symbol','interval','open_time','close_d','high_d','low_d')\n",
    "  .groupBy('exchange','symbol','interval')\n",
    "  .applyInPandas(compute_tech, schema=tech_schema)\n",
    ")\n",
    "\n",
    "feat = feat.join(tech, on=['exchange','symbol','interval','open_time'], how='left')\n",
    "display(feat.select('exchange','symbol','open_dt_syd','macd_hist','bb_bw_20','donch_pos_20','mom_logret_12').limit(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-timeframe context: 1d features joined onto 4h bars\n",
    "We compute a small set of 1d features per symbol and join the **most recent 1d bar at or before** each 4h bar.\n",
    "This provides higher-timeframe trend/regime context without lookahead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build 1d feature table for the same (exchange,symbol) universe\n",
    "# Includes 1d RSI computed per symbol via applyInPandas.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "def _rsi_sma(close: pd.Series, n: int = 14) -> pd.Series:\n",
    "  delta = close.diff()\n",
    "  up = delta.clip(lower=0)\n",
    "  down = (-delta).clip(lower=0)\n",
    "  roll_up = up.rolling(n, min_periods=n).mean()\n",
    "  roll_down = down.rolling(n, min_periods=n).mean()\n",
    "  rs = roll_up / roll_down\n",
    "  return 100 - (100 / (1 + rs))\n",
    "\n",
    "ohlc_1d_base = (spark.table('ohlc')\n",
    "  .join(universe.select('exchange','symbol').distinct(), on=['exchange','symbol'], how='inner')\n",
    "  .where(F.col('interval') == '1d')\n",
    "  .select('exchange','symbol','interval','open_time','high','low','close')\n",
    "  .withColumn('close_d', F.col('close').cast('double'))\n",
    "  .withColumn('high_d', F.col('high').cast('double'))\n",
    "  .withColumn('low_d', F.col('low').cast('double'))\n",
    ")\n",
    "\n",
    "schema_1d = T.StructType([\n",
    "  T.StructField('exchange', T.StringType()),\n",
    "  T.StructField('symbol', T.StringType()),\n",
    "  T.StructField('open_time', T.LongType()),\n",
    "  T.StructField('sma_20d', T.DoubleType()),\n",
    "  T.StructField('sma_50d', T.DoubleType()),\n",
    "  T.StructField('trend_up_1d', T.IntegerType()),\n",
    "  T.StructField('ret_vol_20d', T.DoubleType()),\n",
    "  T.StructField('rsi_14_1d', T.DoubleType()),\n",
    "  T.StructField('macd_hist_1d', T.DoubleType()),\n",
    "  T.StructField('adx_14_1d', T.DoubleType()),\n",
    "])\n",
    "\n",
    "def _ema_1d(s: pd.Series, span: int) -> pd.Series:\n",
    "  return s.ewm(span=span, adjust=False).mean()\n",
    "\n",
    "def _macd_hist_1d(close: pd.Series) -> pd.Series:\n",
    "  macd_line = _ema_1d(close, 12) - _ema_1d(close, 26)\n",
    "  sig = _ema_1d(macd_line, 9)\n",
    "  return macd_line - sig\n",
    "\n",
    "def _adx_1d(high: pd.Series, low: pd.Series, close: pd.Series, n: int = 14) -> pd.Series:\n",
    "  # Wilder ADX on 1d\n",
    "  high_shift = high.shift(1)\n",
    "  low_shift = low.shift(1)\n",
    "  close_shift = close.shift(1)\n",
    "  up_move = high - high_shift\n",
    "  down_move = low_shift - low\n",
    "  plus_dm = np.where((up_move > down_move) & (up_move > 0), up_move, 0.0)\n",
    "  minus_dm = np.where((down_move > up_move) & (down_move > 0), down_move, 0.0)\n",
    "  tr = pd.concat([(high - low).abs(), (high - close_shift).abs(), (low - close_shift).abs()], axis=1).max(axis=1)\n",
    "  atr = tr.ewm(alpha=1/n, adjust=False).mean()\n",
    "  plus_di = 100 * (pd.Series(plus_dm, index=high.index).ewm(alpha=1/n, adjust=False).mean() / atr)\n",
    "  minus_di = 100 * (pd.Series(minus_dm, index=high.index).ewm(alpha=1/n, adjust=False).mean() / atr)\n",
    "  dx = (100 * (plus_di - minus_di).abs() / (plus_di + minus_di)).replace([np.inf, -np.inf], np.nan)\n",
    "  return dx.ewm(alpha=1/n, adjust=False).mean()\n",
    "\n",
    "def compute_1d(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "  pdf = pdf.sort_values('open_time').reset_index(drop=True)\n",
    "  c = pd.to_numeric(pdf['close_d'], errors='coerce')\n",
    "  h = pd.to_numeric(pdf.get('high_d'), errors='coerce')\n",
    "  l = pd.to_numeric(pdf.get('low_d'), errors='coerce')\n",
    "  ret = np.log(c / c.shift(1))\n",
    "  out = pdf[['exchange','symbol','open_time']].copy()\n",
    "  out['sma_20d'] = c.rolling(20, min_periods=20).mean()\n",
    "  out['sma_50d'] = c.rolling(50, min_periods=50).mean()\n",
    "  out['trend_up_1d'] = (out['sma_20d'] > out['sma_50d']).astype('int64')\n",
    "  out['ret_vol_20d'] = ret.rolling(20, min_periods=20).std()\n",
    "  out['rsi_14_1d'] = _rsi_sma(c, 14)\n",
    "  out['macd_hist_1d'] = _macd_hist_1d(c)\n",
    "  out['adx_14_1d'] = _adx_1d(h, l, c, 14)\n",
    "  return out\n",
    "\n",
    "ohlc_1d = (ohlc_1d_base\n",
    "  .groupBy('exchange','symbol')\n",
    "  .applyInPandas(compute_1d, schema=schema_1d)\n",
    ")\n",
    "\n",
    "# Optimized as-of join: only consider candidate 1d bars from within the last 2 days\n",
    "MS_1D = 86400000\n",
    "f4 = feat.alias('f4')\n",
    "d1 = ohlc_1d.alias('d1')\n",
    "j = (f4.join(d1, on=[f4.exchange==d1.exchange, f4.symbol==d1.symbol], how='left')\n",
    "  .where((d1.open_time <= f4.open_time) & (d1.open_time >= (f4.open_time - F.lit(2*MS_1D))))\n",
    ")\n",
    "w_asof = Window.partitionBy('f4.exchange','f4.symbol','f4.interval','f4.open_time').orderBy(F.col('d1.open_time').desc())\n",
    "feat = (j\n",
    "  .withColumn('rn', F.row_number().over(w_asof))\n",
    "  .where(F.col('rn')==1)\n",
    "  .drop('rn')\n",
    "  .withColumnRenamed('sma_20d','sma_20d_1d')\n",
    "  .withColumnRenamed('sma_50d','sma_50d_1d')\n",
    "  .withColumnRenamed('ret_vol_20d','ret_vol_20d_1d')\n",
    ")\n",
    "display(feat.select('exchange','symbol','open_dt_syd','trend_up_1d','rsi_14_1d','sma_20d_1d','sma_50d_1d').limit(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Barrier label (TP hit before SL within W bars)\n",
    "Label is computed from future highs/lows within the next W bars relative to entry (next bar open).\n",
    "\n",
    "Conservative assumptions:\n",
    "- We treat entry as next bar open (`open` at bar_idx+1)\n",
    "- We compute whether any future `high` breaches TP and whether any future `low` breaches SL\n",
    "- Ordering within a bar is ambiguous; for labeling we approximate by first-hit using the earliest bar where each condition occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entry is next bar open. Create an entry view with entry_bar_idx and entry_price\n",
    "entry = (feat\n",
    "  .withColumn('entry_bar_idx', (F.col('bar_idx') + 1).cast('long'))\n",
    "  .withColumn('entry_open', F.lead('open_d', 1).over(w))\n",
    "  .withColumn('atr_entry', F.lead('atr', 1).over(w))\n",
    "  .withColumn('open_dt_utc_entry', F.lead('open_dt_utc', 1).over(w))\n",
    "  .withColumn('open_dt_syd_entry', F.lead('open_dt_syd', 1).over(w))\n",
    "  .where(F.col('entry_open').isNotNull() & F.col('atr_entry').isNotNull())\n",
    "  .withColumn('tp_price', F.col('entry_open') + F.lit(TP_ATR) * F.col('atr_entry'))\n",
    "  .withColumn('sl_price', F.col('entry_open') - F.lit(SL_ATR) * F.col('atr_entry'))\n",
    ")\n",
    "\n",
    "future = (feat\n",
    "  .select('exchange','symbol','interval','bar_idx','high_d','low_d')\n",
    "  .withColumnRenamed('bar_idx','f_bar_idx')\n",
    ")\n",
    "\n",
    "# Join future bars within [entry_bar_idx, entry_bar_idx + W_BARS]\n",
    "joined = (entry\n",
    "  .select('exchange','symbol','interval','bar_idx','entry_bar_idx','entry_open','tp_price','sl_price','open_dt_utc_entry','open_dt_syd_entry',\n",
    "          'hour_syd','dow_syd',\n",
    "          'atr_entry','atrp','trend_up','trend_strength','ret_1','ret_vol','range','body_to_range','upper_wick','lower_wick','gap',\n",
    "          'dist_to_hh_atr','dist_to_ll_atr','vol_z')\n",
    "  .join(future, on=['exchange','symbol','interval'], how='inner')\n",
    "  .where((F.col('f_bar_idx') >= F.col('entry_bar_idx')) & (F.col('f_bar_idx') <= (F.col('entry_bar_idx') + F.lit(W_BARS))))\n",
    ")\n",
    "\n",
    "# Earliest bar that hits TP and earliest bar that hits SL\n",
    "tp_hit = F.when(F.col('high_d') >= F.col('tp_price'), F.col('f_bar_idx'))\n",
    "sl_hit = F.when(F.col('low_d') <= F.col('sl_price'), F.col('f_bar_idx'))\n",
    "\n",
    "hits = (joined\n",
    "  .groupBy('exchange','symbol','interval','bar_idx','entry_bar_idx')\n",
    "  .agg(\n",
    "    F.min(tp_hit).alias('tp_hit_idx'),\n",
    "    F.min(sl_hit).alias('sl_hit_idx'),\n",
    "    F.first('entry_open').alias('entry_open'),\n",
    "    F.first('tp_price').alias('tp_price'),\n",
    "    F.first('sl_price').alias('sl_price'),\n",
    "    F.first('open_dt_utc_entry').alias('entry_dt_utc'),\n",
    "    F.first('open_dt_syd_entry').alias('entry_dt_syd'),\n",
    "    F.first('hour_syd').alias('hour_syd'),\n",
    "    F.first('dow_syd').alias('dow_syd'),\n",
    "    F.first('atr_entry').alias('atr_entry'),\n",
    "    F.first('atrp').alias('atrp'),\n",
    "    F.first('trend_up').alias('trend_up'),\n",
    "    F.first('trend_strength').alias('trend_strength'),\n",
    "    F.first('ret_1').alias('ret_1'),\n",
    "    F.first('ret_vol').alias('ret_vol'),\n",
    "    F.first('range').alias('range'),\n",
    "    F.first('body_to_range').alias('body_to_range'),\n",
    "    F.first('upper_wick').alias('upper_wick'),\n",
    "    F.first('lower_wick').alias('lower_wick'),\n",
    "    F.first('gap').alias('gap'),\n",
    "    F.first('dist_to_hh_atr').alias('dist_to_hh_atr'),\n",
    "    F.first('dist_to_ll_atr').alias('dist_to_ll_atr'),\n",
    "    F.first('vol_z').alias('vol_z')\n",
    "  )\n",
    ")\n",
    "\n",
    "# Label with ambiguity handling\n",
    "# - If TP and SL hit in different bars: whichever hits first wins\n",
    "# - If both hit in the same bar: apply AMBIGUITY_POLICY\n",
    "ambiguous_same_bar = (F.col('tp_hit_idx').isNotNull() & F.col('sl_hit_idx').isNotNull() & (F.col('tp_hit_idx') == F.col('sl_hit_idx')))\n",
    "\n",
    "base_label = (\n",
    "  F.when(F.col('tp_hit_idx').isNotNull() & (F.col('sl_hit_idx').isNull() | (F.col('tp_hit_idx') < F.col('sl_hit_idx'))), F.lit(1))\n",
    "   .when(F.col('sl_hit_idx').isNotNull() & (F.col('tp_hit_idx').isNull() | (F.col('sl_hit_idx') < F.col('tp_hit_idx'))), F.lit(0))\n",
    "   .otherwise(F.lit(None))\n",
    ")\n",
    "\n",
    "label = (\n",
    "  F.when(ambiguous_same_bar & (F.lit(AMBIGUITY_POLICY) == F.lit('tp_first')), F.lit(1))\n",
    "   .when(ambiguous_same_bar & (F.lit(AMBIGUITY_POLICY) == F.lit('sl_first')), F.lit(0))\n",
    "   .when(ambiguous_same_bar & (F.lit(AMBIGUITY_POLICY) == F.lit('discard_both')), F.lit(None))\n",
    "   .otherwise(base_label)\n",
    ")\n",
    "\n",
    "dataset = (hits\n",
    "  .withColumn('ambiguous_same_bar', ambiguous_same_bar.cast('int'))\n",
    "  .withColumn('label', label)\n",
    "  .withColumn('label_timeout', (F.col('label').isNull()).cast('int'))\n",
    "  .withColumn('label_variant', F.concat(F.lit('TP'), F.lit(TP_ATR), F.lit('_SL'), F.lit(SL_ATR), F.lit('_W'), F.lit(W_BARS), F.lit('_'), F.lit(AMBIGUITY_POLICY)))\n",
    "  .withColumn('run_id', F.lit(RUN_ID))\n",
    ")\n",
    "\n",
    "display(dataset.select('exchange','symbol','entry_dt_syd','label','tp_hit_idx','sl_hit_idx','trend_up','dist_to_hh_atr','atrp').limit(20))\n",
    "print('dataset rows:', dataset.count())\n",
    "display(dataset.groupBy('label').count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist dataset to Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "CREATE TABLE IF NOT EXISTS ml_barrier_dataset_4h (\n",
    "  run_id STRING,\n",
    "  exchange STRING,\n",
    "  symbol STRING,\n",
    "  interval STRING,\n",
    "  bar_idx BIGINT,\n",
    "  entry_bar_idx BIGINT,\n",
    "  entry_dt_utc TIMESTAMP,\n",
    "  entry_dt_syd TIMESTAMP,\n",
    "  hour_syd INT,\n",
    "  dow_syd STRING,\n",
    "  entry_open DOUBLE,\n",
    "  tp_price DOUBLE,\n",
    "  sl_price DOUBLE,\n",
    "  tp_hit_idx BIGINT,\n",
    "  sl_hit_idx BIGINT,\n",
    "  ambiguous_same_bar INT,\n",
    "  label INT,\n",
    "  label_timeout INT,\n",
    "  label_variant STRING,\n",
    "  atr_entry DOUBLE,\n",
    "  atrp DOUBLE,\n",
    "  trend_up INT,\n",
    "  trend_strength DOUBLE,\n",
    "  ret_1 DOUBLE,\n",
    "  ret_vol DOUBLE,\n",
    "  range DOUBLE,\n",
    "  body_to_range DOUBLE,\n",
    "  upper_wick DOUBLE,\n",
    "  lower_wick DOUBLE,\n",
    "  gap DOUBLE,\n",
    "  dist_to_hh_atr DOUBLE,\n",
    "  dist_to_ll_atr DOUBLE,\n",
    "  vol_z DOUBLE,\n",
    "  ema_20 DOUBLE,\n",
    "  ema_50 DOUBLE,\n",
    "  ema_ratio DOUBLE,\n",
    "  rsi_14 DOUBLE,\n",
    "  adx_14 DOUBLE,\n",
    "  macd_line DOUBLE,\n",
    "  macd_signal DOUBLE,\n",
    "  macd_hist DOUBLE,\n",
    "  bb_bw_20 DOUBLE,\n",
    "  donch_pos_20 DOUBLE,\n",
    "  mom_logret_3 DOUBLE,\n",
    "  mom_logret_6 DOUBLE,\n",
    "  mom_logret_12 DOUBLE,\n",
    "  sma_20d_1d DOUBLE,\n",
    "  sma_50d_1d DOUBLE,\n",
    "  trend_up_1d INT,\n",
    "  ret_vol_20d_1d DOUBLE,\n",
    "  rsi_14_1d DOUBLE\n",
    ") USING DELTA\n",
    "''')\n",
    "\n",
    "(dataset\n",
    "  .write\n",
    "  .mode('append')\n",
    "  .saveAsTable('ml_barrier_dataset_4h')\n",
    ")\n",
    "print('Wrote run_id', RUN_ID, 'to ml_barrier_dataset_4h')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model (Spark ML GBTClassifier)\n",
    "We use a **time-based split** and train only on rows with non-null labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "df = dataset.where(F.col('label').isNotNull())\n",
    "\n",
    "df = (df\n",
    "  .withColumn('split',\n",
    "    F.when(F.col('entry_dt_utc') < F.to_timestamp(F.lit(TRAIN_END)), F.lit('train'))\n",
    "     .when(F.col('entry_dt_utc') < F.to_timestamp(F.lit(VALID_END)), F.lit('valid'))\n",
    "     .otherwise(F.lit('test'))\n",
    "  )\n",
    ")\n",
    "\n",
    "feature_cols = [\n",
    "  # core\n",
    "  'atr_entry','atrp','trend_up','trend_strength','ret_1','ret_vol','range','body_to_range','upper_wick','lower_wick','gap',\n",
    "  'dist_to_hh_atr','dist_to_ll_atr','vol_z',\n",
    "  # enhanced indicators (applyInPandas)\n",
    "  'ema_20','ema_50','ema_ratio','rsi_14','adx_14',\n",
    "  'macd_line','macd_signal','macd_hist','bb_bw_20','donch_pos_20',\n",
    "  'mom_logret_3','mom_logret_6','mom_logret_12',\n",
    "  # higher timeframe context\n",
    "  'sma_20d_1d','sma_50d_1d','trend_up_1d','ret_vol_20d_1d','rsi_14_1d','macd_hist_1d','adx_14_1d',\n",
    "]\n",
    "cat_cols = ['dow_syd']\n",
    "\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f'{c}_idx', handleInvalid='keep') for c in cat_cols]\n",
    "assembler = VectorAssembler(inputCols=feature_cols + [f'{c}_idx' for c in cat_cols], outputCol='features', handleInvalid='keep')\n",
    "\n",
    "clf = GBTClassifier(featuresCol='features', labelCol='label', maxDepth=5, maxIter=80, stepSize=0.1)\n",
    "pipeline = Pipeline(stages=indexers + [assembler, clf])\n",
    "\n",
    "train = df.where(F.col('split')=='train')\n",
    "valid = df.where(F.col('split')=='valid')\n",
    "test = df.where(F.col('split')=='test')\n",
    "\n",
    "mlflow.autolog()\n",
    "with mlflow.start_run(run_name=f'barrier_4h_{RUN_ID}'):\n",
    "  model = pipeline.fit(train)\n",
    "  pred_valid = model.transform(valid)\n",
    "  pred_test = model.transform(test)\n",
    "\n",
    "  evaluator = BinaryClassificationEvaluator(labelCol='label', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
    "  auc_valid = evaluator.evaluate(pred_valid)\n",
    "  auc_test = evaluator.evaluate(pred_test)\n",
    "  print('AUC valid:', auc_valid)\n",
    "  print('AUC test :', auc_test)\n",
    "\n",
    "  mlflow.log_metric('auc_valid', auc_valid)\n",
    "  mlflow.log_metric('auc_test', auc_test)\n",
    "\n",
    "  mlflow.spark.log_model(model, 'model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "- Spark GBT feature importances\n",
    "- Sampled permutation importance on validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import pandas as pd\n",
    "\n",
    "gbt_model = model.stages[-1]\n",
    "assembler_stage = model.stages[-2]\n",
    "input_cols = assembler_stage.getInputCols()\n",
    "importances = list(gbt_model.featureImportances)\n",
    "imp_df = pd.DataFrame({'feature': input_cols, 'importance': importances}).sort_values('importance', ascending=False)\n",
    "display(imp_df.head(50))\n",
    "\n",
    "perm_sample = valid.select(['label'] + feature_cols + cat_cols).sample(False, 0.05, seed=42)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='label', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
    "base_auc = evaluator.evaluate(model.transform(perm_sample))\n",
    "print('baseline AUC(sampled valid)=', base_auc)\n",
    "\n",
    "K = 15\n",
    "top_feats = imp_df['feature'].head(K).tolist()\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def permute_col(df_in, col):\n",
    "  tmp = df_in.withColumn('_id', F.monotonically_increasing_id())\n",
    "  shuffled = tmp.select(col).withColumn('_rn', F.row_number().over(Window.orderBy(F.rand(123))))\n",
    "  tmp2 = tmp.withColumn('_rn', F.row_number().over(Window.orderBy(F.lit(1))))\n",
    "  out = (tmp2.join(shuffled, on='_rn', how='inner')\n",
    "           .drop(col)\n",
    "           .withColumnRenamed(shuffled.columns[0], col)\n",
    "           .drop('_rn','_id'))\n",
    "  return out\n",
    "\n",
    "rows=[]\n",
    "for f in top_feats:\n",
    "  auc = evaluator.evaluate(model.transform(permute_col(perm_sample, f)))\n",
    "  rows.append((f, float(base_auc-auc), float(auc)))\n",
    "perm_df = pd.DataFrame(rows, columns=['feature','auc_drop','auc_after']).sort_values('auc_drop', ascending=False)\n",
    "display(perm_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score and write predictions to Delta\n",
    "We write predicted probabilities for all labeled rows, plus a simple decision threshold column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score all rows (including train/valid/test for inspection)\n",
    "pred = model.transform(df)\n",
    "\n",
    "# Extract probability of class 1\n",
    "pred = pred.withColumn('p_win', F.col('probability').getItem(1))\n",
    "\n",
    "THRESHOLD = 0.55\n",
    "pred = pred.withColumn('take_trade', (F.col('p_win') >= F.lit(THRESHOLD)).cast('int'))\n",
    "\n",
    "spark.sql('''\n",
    "CREATE TABLE IF NOT EXISTS ml_barrier_predictions_4h (\n",
    "  run_id STRING,\n",
    "  exchange STRING,\n",
    "  symbol STRING,\n",
    "  interval STRING,\n",
    "  entry_dt_utc TIMESTAMP,\n",
    "  entry_dt_syd TIMESTAMP,\n",
    "  split STRING,\n",
    "  label INT,\n",
    "  p_win DOUBLE,\n",
    "  take_trade INT\n",
    ") USING DELTA\n",
    "''')\n",
    "\n",
    "(pred\n",
    "  .select('run_id','exchange','symbol','interval','entry_dt_utc','entry_dt_syd','split','label','p_win','take_trade')\n",
    "  .write\n",
    "  .mode('append')\n",
    "  .saveAsTable('ml_barrier_predictions_4h')\n",
    ")\n",
    "\n",
    "display(pred.select('exchange','symbol','entry_dt_syd','label','p_win','take_trade').orderBy(F.col('entry_dt_utc').desc()).limit(50))\n",
    "print('Wrote predictions for run_id', RUN_ID)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
