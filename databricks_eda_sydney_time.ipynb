{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Databricks EDA — Squeeze Analytics (Sydney time)\n",
    "\n",
    "Assumes your SQLite tables have been loaded into Delta tables (Unity Catalog) as in `copy_into_manual_upload_volume.ipynb`.\n",
    "\n",
    "This notebook focuses on:\n",
    "- quick table sanity checks\n",
    "- timestamp normalization (epoch ms → timestamp)\n",
    "- converting to **Australia/Sydney** (AEST/AEDT DST-aware)\n",
    "- basic distributions across alerts\n",
    "- **OHLC data quality** (gaps/duplicates/invalid bars)\n",
    "- generating a reusable **clean universe** table for backtests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "-- ---- CONFIG ----\n",
    "USE CATALOG `workspace`;\n",
    "USE SCHEMA `squeeze`;\n",
    "SHOW TABLES;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row counts" 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT 'ohlc' AS table, COUNT(*) AS n FROM ohlc\n",
    "UNION ALL SELECT 'alerts', COUNT(*) FROM alerts\n",
    "UNION ALL SELECT 'trade_plans', COUNT(*) FROM trade_plans\n",
    "UNION ALL SELECT 'backtest_trades', COUNT(*) FROM backtest_trades\n",
    "UNION ALL SELECT 'backtest_results', COUNT(*) FROM backtest_results\n",
    "UNION ALL SELECT 'snapshot_cache', COUNT(*) FROM snapshot_cache\n",
    "UNION ALL SELECT 'market_cap_cache', COUNT(*) FROM market_cap_cache\n",
    "ORDER BY n DESC;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestamp normalization patterns (epoch ms → UTC timestamp → Sydney time)\n",
    "\n",
    "In Spark/Databricks:\n",
    "- `to_timestamp(ts/1000)` converts epoch seconds to a timestamp\n",
    "- `from_utc_timestamp(..., 'Australia/Sydney')` converts a UTC timestamp to Sydney local time\n",
    "\n",
    "These functions are DST-aware." 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Alerts: ts and created_ts in Sydney time\n",
    "SELECT\n",
    "  exchange, symbol, signal, source_tf,\n",
    "  ts,\n",
    "  to_timestamp(ts/1000) AS ts_utc,\n",
    "  from_utc_timestamp(to_timestamp(ts/1000), 'Australia/Sydney') AS ts_sydney,\n",
    "  created_ts,\n",
    "  from_utc_timestamp(to_timestamp(created_ts/1000), 'Australia/Sydney') AS created_ts_sydney\n",
    "FROM alerts\n",
    "ORDER BY ts DESC\n",
    "LIMIT 50;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alert distributions" 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT signal, COUNT(*) AS n\n",
    "FROM alerts\n",
    "GROUP BY signal\n",
    "ORDER BY n DESC\n",
    "LIMIT 50;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT source_tf, COUNT(*) AS n\n",
    "FROM alerts\n",
    "GROUP BY source_tf\n",
    "ORDER BY n DESC\n",
    "LIMIT 50;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day-of-week and hour-of-day in Sydney time" 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "WITH x AS (\n",
    "  SELECT\n",
    "    from_utc_timestamp(to_timestamp(ts/1000), 'Australia/Sydney') AS ts_syd\n",
    "  FROM alerts\n",
    ")\n",
    "SELECT\n",
    "  date_format(ts_syd, 'E') AS dow_syd,\n",
    "  hour(ts_syd) AS hour_syd,\n",
    "  COUNT(*) AS n\n",
    "FROM x\n",
    "GROUP BY 1,2\n",
    "ORDER BY 1,2;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OHLC Data Quality → Clean Universe\n",
    "We’ll compute integrity metrics per `(exchange, symbol, interval)` and write a `clean_universe` table.\n",
    "\n",
    "Checks include:\n",
    "- duplicate bars (same open_time)\n",
    "- time gaps (open_time not equal to previous + expected interval ms)\n",
    "- invalid OHLC (high < max(open,close), low > min(open,close), low > high)\n",
    "\n",
    "Then we filter to a clean set suitable for backtesting." 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Universe evaluation scope\n",
    "INTERVALS = ['5m','15m','1h','4h','1d']\n",
    "MIN_BARS = 2000\n",
    "MAX_GAP_FRAC = 0.001  # allow 0.1% gaps\n",
    "MAX_DUP_FRAC = 0.0001\n",
    "MAX_INVALID_FRAC = 0.0001\n",
    "\n",
    "INTERVAL_MS = {\n",
    "  '1m': 60_000,\n",
    "  '3m': 180_000,\n",
    "  '5m': 300_000,\n",
    "  '15m': 900_000,\n",
    "  '30m': 1_800_000,\n",
    "  '1h': 3_600_000,\n",
    "  '2h': 7_200_000,\n",
    "  '4h': 14_400_000,\n",
    "  '6h': 21_600_000,\n",
    "  '8h': 28_800_000,\n",
    "  '12h': 43_200_000,\n",
    "  '1d': 86_400_000,\n",
    "}\n",
    "\n",
    "interval_ms_expr = F.create_map([F.lit(x) for kv in INTERVAL_MS.items() for x in kv])\n",
    "\n",
    "ohlc0 = (spark.table('ohlc')\n",
    "  .where(F.col('interval').isin(INTERVALS))\n",
    "  .select('exchange','symbol','interval','open_time','open','high','low','close','volume')\n",
    "  .withColumn('expected_ms', interval_ms_expr[F.col('interval')])\n",
    ")\n",
    "\n",
    "w = Window.partitionBy('exchange','symbol','interval').orderBy('open_time')\n",
    "ohlc1 = (ohlc0\n",
    "  .withColumn('prev_open_time', F.lag('open_time').over(w))\n",
    "  .withColumn('dt', F.col('open_time') - F.col('prev_open_time'))\n",
    "  .withColumn('is_gap', (F.col('prev_open_time').isNotNull()) & (F.col('dt') != F.col('expected_ms')))\n",
    "  .withColumn('is_dup', (F.col('prev_open_time').isNotNull()) & (F.col('dt') == F.lit(0)))\n",
    "  .withColumn('invalid_ohlc', (F.col('low') > F.col('high')) | (F.col('high') < F.greatest(F.col('open'), F.col('close'))) | (F.col('low') > F.least(F.col('open'), F.col('close'))))\n",
    ")\n",
    "\n",
    "quality = (ohlc1\n",
    "  .groupBy('exchange','symbol','interval','expected_ms')\n",
    "  .agg(\n",
    "    F.count('*').alias('bars'),\n",
    "    F.sum(F.col('is_gap').cast('long')).alias('gap_bars'),\n",
    "    F.sum(F.col('is_dup').cast('long')).alias('dup_bars'),\n",
    "    F.sum(F.col('invalid_ohlc').cast('long')).alias('invalid_bars'),\n",
    "    F.min('open_time').alias('min_open_time'),\n",
    "    F.max('open_time').alias('max_open_time'),\n",
    "  )\n",
    "  .withColumn('gap_frac', F.col('gap_bars')/F.col('bars'))\n",
    "  .withColumn('dup_frac', F.col('dup_bars')/F.col('bars'))\n",
    "  .withColumn('invalid_frac', F.col('invalid_bars')/F.col('bars'))\n",
    "  .orderBy(F.col('bars').desc())\n",
    ")\n",
    "\n",
    "display(quality.limit(200))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and write `clean_universe`\n",
    "The clean universe is the subset that passes our quality thresholds." 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_universe = (quality\n",
    "  .where(F.col('bars') >= F.lit(MIN_BARS))\n",
    "  .where(F.col('gap_frac') <= F.lit(MAX_GAP_FRAC))\n",
    "  .where(F.col('dup_frac') <= F.lit(MAX_DUP_FRAC))\n",
    "  .where(F.col('invalid_frac') <= F.lit(MAX_INVALID_FRAC))\n",
    "  .withColumn('min_open_dt_utc', F.to_timestamp(F.col('min_open_time')/1000))\n",
    "  .withColumn('max_open_dt_utc', F.to_timestamp(F.col('max_open_time')/1000))\n",
    "  .withColumn('min_open_dt_syd', F.from_utc_timestamp(F.col('min_open_dt_utc'), 'Australia/Sydney'))\n",
    "  .withColumn('max_open_dt_syd', F.from_utc_timestamp(F.col('max_open_dt_utc'), 'Australia/Sydney'))\n",
    ")\n",
    "\n",
    "spark.sql('''\n",
    "CREATE TABLE IF NOT EXISTS clean_universe (\n",
    "  exchange STRING,\n",
    "  symbol STRING,\n",
    "  interval STRING,\n",
    "  expected_ms BIGINT,\n",
    "  bars BIGINT,\n",
    "  gap_bars BIGINT,\n",
    "  dup_bars BIGINT,\n",
    "  invalid_bars BIGINT,\n",
    "  gap_frac DOUBLE,\n",
    "  dup_frac DOUBLE,\n",
    "  invalid_frac DOUBLE,\n",
    "  min_open_time BIGINT,\n",
    "  max_open_time BIGINT,\n",
    "  min_open_dt_utc TIMESTAMP,\n",
    "  max_open_dt_utc TIMESTAMP,\n",
    "  min_open_dt_syd TIMESTAMP,\n",
    "  max_open_dt_syd TIMESTAMP\n",
    ") USING DELTA\n",
    "''')\n",
    "\n",
    "# Overwrite each time so downstream notebooks always reference latest clean set\n",
    "(clean_universe\n",
    "  .select('exchange','symbol','interval','expected_ms','bars','gap_bars','dup_bars','invalid_bars','gap_frac','dup_frac','invalid_frac',\n",
    "          'min_open_time','max_open_time','min_open_dt_utc','max_open_dt_utc','min_open_dt_syd','max_open_dt_syd')\n",
    "  .write\n",
    "  .mode('overwrite')\n",
    "  .option('overwriteSchema','true')\n",
    "  .saveAsTable('clean_universe')\n",
    ")\n",
    "\n",
    "display(clean_universe.orderBy(F.col('bars').desc()).limit(200))\n",
    "print('clean_universe count =', clean_universe.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick OHLC preview with Sydney time" 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  exchange, symbol, interval,\n",
    "  open_time,\n",
    "  from_utc_timestamp(to_timestamp(open_time/1000), 'Australia/Sydney') AS open_time_syd,\n",
    "  open, high, low, close, volume\n",
    "FROM ohlc\n",
    "ORDER BY open_time DESC\n",
    "LIMIT 50;\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
